# -*- coding: utf-8 -*-
"""FinalProjectCode_ResearchTopics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-MbT3wrIPZRzY-Y7lIDRN3eqN8uBWW0I
"""

!pip install pyldavis
!pip install -U gensim
!pip install bertopic

import gensim
import gensim.corpora as corpora
import io
import matplotlib.colors as mcolors
import nltk
import numpy as np
import operator
import pandas as pd
import pickle
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
import seaborn as sns
import spacy

from bertopic import BERTopic
from gensim.models import TfidfModel
from gensim.models import CoherenceModel
from gensim.models import Phrases
from gensim.models.phrases import Phraser
from gensim.utils import simple_preprocess
from matplotlib import pyplot as plt
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud, STOPWORDS

nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nlp = spacy.load('en_core_web_sm')

#load a previous built model
import pickle
model = pickle.load(open("lda.model2020", 'rb'))
corpus = pickle.load(open("corpus2020.file", 'rb'))
id2word = pickle.load(open("id2word2020.file", 'rb'))

"""#### Functions for all"""

#"""https://spacy.io/api/annotation"""
#Function all words in our vocabulary to their root form
def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent))
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

#Function to build bigrams
def buildBigrams(bigram_phraser, words):
    return [bigram_phraser[word] for word in words]

#Function to build trigrams
def buildTrigrams(trigram_phraser, bigram_phraser, words):
  return [trigram_phraser[bigram_phraser[word]] for word in words]

#Function to build an LDA model given a corpus, id2word dictionary, number of topics, alpha, and eta
def buildModel(corpus, id2word, topics, alpha, eta):
    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=topics,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=15,
                                           alpha=alpha,
                                           eta=eta)
    return lda_model

#Build a list of stop word
def getStopWords():
  stop_words = stopwords.words('english')
  stop_words.extend(['study', 'project', 'provided', 'utilized', 'also', 'whether',
                    'much', 'based', 'used', 'using', 'many', 'abstract', 'program',
                    'university', 'rochester', 'research', 'develop', 'evaluate',
                   'caused', 'allows', 'requires', 'includes', 'recent'])
  return stop_words

#Function to remove all punctuation in the abstract data
def removePunctuation(abstracts):
  words = []
  for a in abstracts:
      #Remove punctuation
      temp = gensim.utils.simple_preprocess(str(a), deacc = True)
      words.append(temp)
  return words

#Function to filter out words we dont not want in our final model
def filterWords(words, stop_words):
  data = []
  for wordList in words:
      temp = []
      for word in wordList:
        #removing words in stop words and words that are less than 3 characters
          if word not in stop_words and len(word) >= 3:
              temp.append(word)
      data.append(temp)
  return data

#Function to get the bigrams
def getBigrams(data):
  #higher threshold results in fewer n-gram phrases
  bigram = Phrases(data, min_count=5, threshold=2, delimiter= ' ')
  bigram_phraser = Phraser(bigram)
  bigrams = buildBigrams(bigram_phraser, data)
  return bigram, bigram_phraser, bigrams

#Function to get the trigrams
def getTrigrams(bigram, bigram_phraser, data):
  trigram = Phrases(bigram[data], threshold = 5) 
  trigram_phraser = Phraser(trigram)
  trigrams = buildTrigrams(trigram_phraser, bigram_phraser, data)
  return trigrams

#Building our corpus based on id2word dictionary
def buildCorpus(data):
  corpus = []
  for text in data:
    #convert each text to a "bag-of-words"
    new = id2word.doc2bow(text)
    corpus.append(new)
  return corpus

#TF-IDF removal : removes words that are too frequent
def termFrequencyInverse(corpus, id2word):
  tfidf = TfidfModel(corpus, id2word = id2word)
  low_value = 0.03
  words = []
  words_missing_in_tfidf = []
  for i in range(0, len(corpus)):
    bow = corpus[i]
    low_value_words = []
    tfidf_ids = [id for id, value in tfidf[bow]] 
    bow_ids = [id for id, value in bow]
    low_value_words = [id for id, value in tfidf[bow] if value<low_value]
    drops = low_value_words + words_missing_in_tfidf
    for item in drops:
      words.append(id2word[item])
    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]
    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]
    corpus[i] = new_bow
  return corpus

# Dominant topic and it's percentage in each document
def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row_list in enumerate(ldamodel[corpus]):
        row = row_list[0] if ldamodel.per_word_topics else row_list            
        # print(row)
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)

"""# UR data 2000-2020"""

df = pd.read_csv('Dimensions.csv', header = 1)

df = df.dropna(subset=['Abstract'])

#Convert Abstract column to list
abstracts = df["Abstract"].values.tolist()

#Get year data for Topics Over Time chart
year = df['Start Year']

#Next few steps for Preprocessing the data
words = removePunctuation(abstracts)

stop_words = getStopWords()

data = filterWords(words, stop_words)

bigram, bigram_phraser, bigrams = getBigrams(data)

trigrams = getTrigrams(bigram, bigram_phraser, data)

#keep on words that are nouns, adjectives, verbs, or adverbs
data = lemmatization(trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

#Create dictionary mapping each unique word to an id
id2word = corpora.Dictionary(data)

#Create unique id for each word
corpus = buildCorpus(data)

#Cleaning our corpus with terms that are too frequent between documents
corpus = termFrequencyInverse(corpus, id2word)

#Determine best number of topics
k = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
scores = []

for i in k:
  model = buildModel(corpus, id2word, i, 'auto', 'auto')
  coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
  coherence_lda = coherence_model_lda.get_coherence()
  scores.append(coherence_lda)

#Plot for best number of topics
plt.plot(k, scores)
plt.title("Coherence Score based on Number of Topics")
plt.xlabel("Number of topics")
plt.ylabel("Coherence Score")
plt.show()

#Best model
model = buildModel(corpus, id2word, 4, 'auto', 'auto')
coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
coherence_lda

### Looking at dominant keywords under each topic to assign labels in report
df_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=data)

# Format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']
df_dominant_topic.head(10)

for t in range(model.num_topics):
    plt.figure(figsize = (5,5))
    plt.imshow(WordCloud(background_color='white', prefer_horizontal=1).fit_words(dict(model.show_topic(t, 25))))
    plt.axis("off")
    plt.title("Topic #" + str(t+1), fontsize=50)
    plt.show()

pyLDAvis.enable_notebook()
LDAvis_prepared = gensimvis.prepare(model, corpus, id2word) #alpha =0.31 #eta = 0.91
LDAvis_prepared

"""Topic names: 1. Cell Biology 2. Physical/Natural Sciences 3. Clinical research 4. Brain and Cognitive Sciences"""

#Building BERTopic model allowing up to trigrams
vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words = getStopWords())
topic_model = BERTopic(verbose=True, vectorizer_model = vectorizer_model, embedding_model=("paraphrase-MiniLM-L12-v2"), min_topic_size=20)
topics, _ = topic_model.fit_transform(abstracts)

#Getting the top 4 topics from our BERTopic model
top = topic_model.get_topic_info()
top[1:5]

#Total topics found by BERT
len(topic_model.get_topic_info())

#Word cloud for BERT
for t in range(4):
    plt.figure(figsize = (5,5))
    plt.imshow(WordCloud(background_color='white', prefer_horizontal=1).fit_words(dict(topic_model.get_topic(t))))
    plt.axis("off")
    plt.title("Topic #" + str(t+1), fontsize=30)
    plt.show()

#Build topics over time for visualization
topics_over_time = topic_model.topics_over_time(abstracts, topics, year)

topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=4)

topic_model.visualize_topics_over_time(topics_over_time)

#Display similarity matrix to see which topics are similar 
topic_model.visualize_heatmap(n_clusters=5)

#Dump our models to a file for loading later
pickle.dump(model,open("lda.model",'wb'))
pickle.dump(corpus,open("corpus.file",'wb'))
pickle.dump(id2word,open("id2word.file",'wb'))

pickle.dump(topic_model,open("BERT.model",'wb'))

pyLDAvis.save_html(LDAvis_prepared,'URLDA.html')

"""# 2020 UR data"""

df = pd.read_csv('Dimensions.csv', header = 1)

df = df[df['Start Year'] == 2020]
df = df.dropna(subset=["Abstract"])
abstracts = df["Abstract"].values.tolist()

stop_words = getStopWords()

words = removePunctuation(abstracts)

data = filterWords(words, stop_words)

bigram, bigram_phraser, bigrams = getBigrams(data)

trigrams = getTrigrams(bigram, bigram_phraser, data)

data = lemmatization(trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

id2word = corpora.Dictionary(data)

corpus = buildCorpus(data)

corpus = termFrequencyInverse(corpus, id2word)

k = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
scores = []

for i in k:
  model = buildModel(corpus, id2word, i, 'auto', 'auto')
  coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
  coherence_lda = coherence_model_lda.get_coherence()
  scores.append(coherence_lda)

#Plot for best number of topics
plt.plot(k, scores)
plt.title("Coherence Score based on Number of Topics")
plt.xlabel("Number of topics")
plt.ylabel("Coherence Score")
plt.show()

model = buildModel(corpus, id2word, 4, 'auto', 'auto')
coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
coherence_lda

for t in range(model.num_topics):
    plt.figure(figsize = (5,5))
    plt.imshow(WordCloud(background_color='white', prefer_horizontal=1).fit_words(dict(model.show_topic(t, 25))))
    plt.axis("off")
    plt.title("Topic #" + str(t+1), fontsize=50)
    plt.show()

pyLDAvis.enable_notebook()
LDAvis_prepared = gensimvis.prepare(model, corpus, id2word)
LDAvis_prepared

pickle.dump(model,open("lda.model2020",'wb'))
pickle.dump(corpus,open("corpus2020.file",'wb'))
pickle.dump(id2word,open("id2word2020.file",'wb'))

pyLDAvis.save_html(LDAvis_prepared,'URLDA2020.html')

"""# 2015-2020 UR data"""

df = pd.read_csv('Dimensions.csv', header = 1)

df = df[df['Start Year']>=2015]
df = df.dropna(subset=["Abstract"])
abstracts = df["Abstract"].values.tolist()

stop_words = getStopWords()

words = removePunctuation(abstracts)

data = filterWords(words, stop_words)

bigram, bigram_phraser, bigrams = getBigrams(data)

trigrams = getTrigrams(bigram, bigram_phraser, data)

data = lemmatization(trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

id2word = corpora.Dictionary(data)

corpus = buildCorpus(data)

corpus = termFrequencyInverse(corpus, id2word)

k = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
scores = []

for i in k:
  model = buildModel(corpus, id2word, i, 'auto', 'auto')
  coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
  coherence_lda = coherence_model_lda.get_coherence()
  scores.append(coherence_lda)

plt.plot(k, scores)
plt.title("Coherence Score based on Number of Topics")
plt.xlabel("Number of topics")
plt.ylabel("Coherence Score")
plt.show()

model = buildModel(corpus, id2word, 4, 'auto', 'auto')
coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
coherence_lda

for t in range(model.num_topics):
    plt.figure(figsize = (5,5))
    plt.imshow(WordCloud(background_color='white', prefer_horizontal=1).fit_words(dict(model.show_topic(t, 25))))
    plt.axis("off")
    plt.title("Topic #" + str(t+1), fontsize=50)
    plt.show()

pyLDAvis.enable_notebook()
LDAvis_prepared = gensimvis.prepare(model, corpus, id2word)
LDAvis_prepared

df_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=data)

df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']
df_dominant_topic.head(10)

pickle.dump(model,open("lda.model2015_20",'wb'))
pickle.dump(corpus,open("corpus2015_20.file",'wb'))
pickle.dump(id2word,open("id2word2015_20.file",'wb'))

pyLDAvis.save_html(LDAvis_prepared,'URLDA2015_20.html')

"""# 2010 - 2014 UR Data"""

df = pd.read_csv('Dimensions.csv', header = 1)

df = df.loc[(df['Start Year']>=2010)&(df['Start Year']<2015)]
df = df.dropna(subset=["Abstract"])
abstracts = df["Abstract"].values.tolist()

stop_words = getStopWords()

words = removePunctuation(abstracts)

data = filterWords(words, stop_words)

bigram, bigram_phraser, bigrams = getBigrams(data)

trigrams = getTrigrams(bigram, bigram_phraser, data)

data = lemmatization(trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

id2word = corpora.Dictionary(data)

corpus = buildCorpus(data)

corpus = termFrequencyInverse(corpus, id2word)

loaded_model = pickle.load(open("lda.model2010_14", 'rb'))
loaded_corpus = pickle.load(open("corpus.file", 'rb'))
loaded_id2word = pickle.load(open("id2word.file", 'rb'))

k = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
scores = []

for i in k:
  model = buildModel(corpus, id2word, i, 'auto', 'auto')
  coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
  coherence_lda = coherence_model_lda.get_coherence()
  scores.append(coherence_lda)

plt.plot(k, scores)
plt.title("Coherence Score based on Number of Topics")
plt.xlabel("Number of topics")
plt.ylabel("Coherence Score")
plt.show()

model = buildModel(corpus, id2word, 3, 'auto', 'auto')
coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
coherence_lda

for t in range(model.num_topics):
    plt.figure(figsize = (5,5))
    plt.imshow(WordCloud(background_color='white', prefer_horizontal=1).fit_words(dict(model.show_topic(t, 25))))
    plt.axis("off")
    plt.title("Topic #" + str(t+1), fontsize=50)
    plt.show()

pyLDAvis.enable_notebook()
LDAvis_prepared = gensimvis.prepare(model, corpus, id2word)
LDAvis_prepared

df_topic_sents_keywords = format_topics_sentences(ldamodel=loaded_model, corpus=loaded_corpus, texts=loaded_id2word)

df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']
df_dominant_topic.head(10)

pickle.dump(model,open("lda.model2010_14",'wb'))
pickle.dump(corpus,open("corpus2010_14.file",'wb'))
pickle.dump(id2word,open("id2word2010_14.file",'wb'))

pyLDAvis.save_html(LDAvis_prepared,'URLDA2010_14.html')

"""# 2005-2009 UR Data"""

df = pd.read_csv('Dimensions.csv', header = 1)

df = df.loc[(df['Start Year']>=2005)&(df['Start Year']<2010)]
df = df.dropna(subset=["Abstract"])
abstracts = df["Abstract"].values.tolist()

stop_words = getStopWords()

words = removePunctuation(abstracts)

data = filterWords(words, stop_words)

bigram, bigram_phraser, bigrams = getBigrams(data)

trigrams = getTrigrams(bigram, bigram_phraser, data)

data = lemmatization(trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

id2word = corpora.Dictionary(data)

corpus = buildCorpus(data)

corpus = termFrequencyInverse(corpus, id2word)

k = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
scores = []

for i in k:
  model = buildModel(corpus, id2word, i, 'auto', 'auto')
  coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
  coherence_lda = coherence_model_lda.get_coherence()
  scores.append(coherence_lda)

plt.plot(k, scores)
plt.title("Coherence Score based on Number of Topics")
plt.xlabel("Number of topics")
plt.ylabel("Coherence Score")
plt.show()

model = buildModel(corpus, id2word, 4, 'auto', 'auto') 
coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
coherence_lda

for t in range(model.num_topics):
    plt.figure(figsize = (5,5))
    plt.imshow(WordCloud(background_color='white', prefer_horizontal=1).fit_words(dict(model.show_topic(t, 25))))
    plt.axis("off")
    plt.title("Topic #" + str(t+1), fontsize=50)
    plt.show()

pyLDAvis.enable_notebook()
LDAvis_prepared = gensimvis.prepare(model, corpus, id2word)
LDAvis_prepared

pickle.dump(model,open("lda.model2005_09",'wb'))
pickle.dump(corpus,open("corpus2005_09.file",'wb'))
pickle.dump(id2word,open("id2word2005_09.file",'wb'))

pyLDAvis.save_html(LDAvis_prepared,'URLDA2005_09.html')

"""# 2000 - 2004 UR Data"""

df = pd.read_csv('Dimensions.csv', header = 1)

df = df.loc[(df['Start Year']>=2000)&(df['Start Year']<2005)]
df = df.dropna(subset=["Abstract"])
abstracts = df["Abstract"].values.tolist()

stop_words = getStopWords()

words = removePunctuation(abstracts)

data = filterWords(words, stop_words)

bigram, bigram_phraser, bigrams = getBigrams(data)

trigrams = getTrigrams(bigram, bigram_phraser, data)

data = lemmatization(trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

id2word = corpora.Dictionary(data)

corpus = buildCorpus(data)

corpus = termFrequencyInverse(corpus, id2word)

k = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
scores = []

for i in k:
  model = buildModel(corpus, id2word, i, 'auto', 'auto')
  coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
  coherence_lda = coherence_model_lda.get_coherence()
  scores.append(coherence_lda)

plt.plot(k, scores)
plt.title("Coherence Score based on Number of Topics")
plt.xlabel("Number of topics")
plt.ylabel("Coherence Score")
plt.show()

model = buildModel(corpus, id2word, 3, 'auto', 'auto') 
coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
coherence_lda

for t in range(model.num_topics):
    plt.figure(figsize = (5,5))
    plt.imshow(WordCloud(background_color='white', prefer_horizontal=1).fit_words(dict(model.show_topic(t, 25))))
    plt.axis("off")
    plt.title("Topic #" + str(t+1), fontsize=50)
    plt.show()

pyLDAvis.enable_notebook()
LDAvis_prepared = gensimvis.prepare(model, corpus, id2word)
LDAvis_prepared

pickle.dump(model,open("lda.model2000_04",'wb'))
pickle.dump(corpus,open("corpus2000_04.file",'wb'))
pickle.dump(id2word,open("id2word2000_04.file",'wb'))

pyLDAvis.save_html(LDAvis_prepared,'URLDA2005_09.html')

"""# 2000 UR data

"""

df = pd.read_csv('Dimensions.csv', header = 1)

df = df[df['Start Year'] == 2000]
df = df.dropna(subset=["Abstract"])
abstracts = df["Abstract"].values.tolist()

stop_words = getStopWords()

words = removePunctuation(abstracts)

words = removePunctuation(abstracts)

bigram, bigram_phraser, bigrams = getBigrams(data)

trigrams = getTrigrams(bigram, bigram_phraser, data)

data = lemmatization(trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

id2word = corpora.Dictionary(data)

corpus = buildCorpus(data)

corpus = termFrequencyInverse(corpus, id2word)

k = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
scores = []

for i in k:
  model = buildModel(corpus, id2word, i, 'auto', 'auto')
  coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
  coherence_lda = coherence_model_lda.get_coherence()
  scores.append(coherence_lda)

plt.plot(k, scores)
plt.title("Coherence Score based on Number of Topics")
plt.xlabel("Number of topics")
plt.ylabel("Coherence Score")
plt.show()

model = buildModel(corpus, id2word, 3, 'auto', 'auto')
coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
coherence_lda

for t in range(model.num_topics):
    plt.figure(figsize = (5,5))
    plt.imshow(WordCloud(background_color='white', prefer_horizontal=1).fit_words(dict(model.show_topic(t, 25))))
    plt.axis("off")
    plt.title("Topic #" + str(t+1), fontsize=50)
    plt.show()

pyLDAvis.enable_notebook()
LDAvis_prepared = gensimvis.prepare(model, corpus, id2word)
LDAvis_prepared

pickle.dump(model,open("lda.model2000",'wb'))
pickle.dump(corpus,open("corpus2000.file",'wb'))
pickle.dump(id2word,open("id2word2000.file",'wb'))

pyLDAvis.save_html(LDAvis_prepared,'URLDA2000.html')

"""# CS grants at R1 institutions

"""

model = pickle.load(open("r1lda.model", 'rb'))
loaded_corpus = pickle.load(open("r1corpus.file", 'rb'))
loaded_id2word = pickle.load(open("r1id2word.file", 'rb'))

df = pd.read_csv('r1data.csv')
df = df.dropna(subset=['Abstract'])

abstracts = df["Abstract"].values.tolist()

year = df['Start Year']

stop_words = getStopWords()

words = removePunctuation(abstracts)

data = filterWords(words, stop_words)

bigram, bigram_phraser, bigrams = getBigrams(data)

trigrams = getTrigrams(bigram, bigram_phraser, data)

data = lemmatization(trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

id2word = corpora.Dictionary(data)

corpus = buildCorpus(data)

corpus = termFrequencyInverse(corpus, id2word)

#Calculate best k in batches
k = [3,6,9,12,15] 
scores = []

for i in k:
  model = buildModel(corpus, id2word, i, 'auto', 'auto')
  coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
  coherence_lda = coherence_model_lda.get_coherence()
  scores.append(coherence_lda)

k = [18,21,24,27,30] 

for i in k:
  model = buildModel(corpus, id2word, i, 'auto', 'auto')
  coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
  coherence_lda = coherence_model_lda.get_coherence()
  scores.append(coherence_lda)

k = [3,6,9,12,15,18,21,24,27,30] 
plt.plot(k, scores)
plt.title("Coherence Score based on Number of Topics")
plt.xlabel("Number of topics")
plt.ylabel("Coherence Score")
plt.show()

#Best model for R1 data
model = buildModel(corpus, id2word, 15, 'auto', 'auto')
coherence_model_lda = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
coherence_lda

df_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=data)

df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']
df_dominant_topic.head(10)

for t in range(model.num_topics):
    plt.figure(figsize = (5,5))
    plt.imshow(WordCloud(background_color='white', prefer_horizontal=1).fit_words(dict(model.show_topic(t, 25))))
    plt.axis("off")
    plt.title("Topic #" + str(t+1), fontsize=30)
    plt.show()

pyLDAvis.enable_notebook()
LDAvis_prepared = gensimvis.prepare(model, corpus, id2word)
LDAvis_prepared

pickle.dump(model,open("r1lda.model",'wb'))
pickle.dump(corpus,open("r1corpus.file",'wb'))
pickle.dump(id2word,open("r1id2word.file",'wb'))
pyLDAvis.save_html(LDAvis_prepared,'r1model.html')

"""BERTOPIC with vectorizer, embedded and stop words"""

vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words = stopwords.words('english'), min_df=15)

topic_model = BERTopic(verbose=True, vectorizer_model =vectorizer_model, embedding_model=("paraphrase-MiniLM-L12-v2"), min_topic_size=200, calculate_probabilities = "FALSE")
topics, _ = topic_model.fit_transform(abstracts); len(topic_model.get_topic_info())  #we get 36 topics

pickle.dump(topic_model,open("r1bert.model",'wb'))

topics_over_time = topic_model.topics_over_time(abstracts, topics, year)

topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=15, width=900, height=500)

topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=36, width=900, height=500)

topic_model.visualize_heatmap(top_n_topics=36, n_clusters=5)

topic_model.get_topic(4) #topics 4, 16, 26 and 32 have high similarities

topic_model.get_topic(16)

topic_model.get_topic(26)

topic_model.get_topic(32)

top = topic_model.get_topic_info()
top

for t in range(15):
    plt.figure(figsize = (10,10))
    plt.imshow(WordCloud(background_color='white', prefer_horizontal=1).fit_words(dict(topic_model.get_topic(t))))
    plt.axis("off")
    plt.title("Topic #" + str(t+1), fontsize=30)
    plt.show()